{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T02:50:48.956922Z",
     "start_time": "2024-12-17T02:50:48.955287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DATASET = \"swedish_tunes.csv\"\n",
    "TRANSLATION_FILE = \"translation\"\n",
    "INPUT_SIZE = 5\n",
    "HIDDEN_LAYERS = 0\n",
    "NUM_LAYERS = 0"
   ],
   "id": "2812edcfc4a46310",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas\n",
    "from torch.nn import RNN, Linear, LogSoftmax, Module\n",
    "import torch\n",
    "import argparse\n",
    "import math"
   ],
   "id": "c9bbac2d5f247657"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Class that creates the model that we'll use\n",
    "class PredictRNN(Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PredictRNN, self).__init__()\n",
    "\n",
    "        self.rnn = RNN(input_size, hidden_size)\n",
    "        self.h2o = Linear(hidden_size, output_size)\n",
    "        self.softmax = LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, line_tensor):\n",
    "        rnn_out, hidden = self.rnn(line_tensor)\n",
    "        output = self.h2o(hidden[0])\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ],
   "id": "8aaefdaaa4f499a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Performs one-hot encoding on a single token vector\n",
    "def onehot(line, n_tokens, translation):\n",
    "    tensor = torch.zeros(len(line), 1, n_tokens)\n",
    "    for ind, token in enumerate(line):\n",
    "        tensor[ind][0][translation.find(token)] = 1\n",
    "    return tensor\n"
   ],
   "id": "c3494db4ace98f28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T02:52:30.275672Z",
     "start_time": "2024-12-17T02:52:30.273594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get min number of tokens in a line\n",
    "def min_line_size(filepath):\n",
    "    file_p = open(filepath, 'r', encoding='latin-1')\n",
    "    min_tokens = float('inf')\n",
    "    for line in file_p.readlines():\n",
    "        line_tok = line.split(',')\n",
    "        line_len = len(line_tok)\n",
    "        if line_len < min_tokens:\n",
    "            min_tokens = line_len\n",
    "    file_p.close()\n",
    "    return min_tokens\n"
   ],
   "id": "23330daea7d29a6a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# calculates the accuracy of the predictions of a neural network\n",
    "# For classification tasks\n",
    "# NOTE: X and y should already be PyTorch tensors\n",
    "def calculate_accuracy(network, X, y):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "    # make predictions for the given X\n",
    "    probs = network(X)\n",
    "    predictions = torch.argmax(probs, dim=1)\n",
    "    # the calculate accuracy of those predictions\n",
    "    total = len(predictions)\n",
    "    accuracy = sum(predictions == y) / total\n",
    "    variance = accuracy * (1 - accuracy)\n",
    "    std_err = math.sqrt(variance / total)\n",
    "    up_bound = accuracy + 2.39 * std_err\n",
    "    low_bound = accuracy - 2.39 * std_err\n",
    "    return float(accuracy), up_bound, low_bound\n"
   ],
   "id": "d11837415b017597"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# converts a training set into smaller train and validation sets\n",
    "def create_validation(training_X, training_y, valid_percentage):\n",
    "    # find the split point between training and validation\n",
    "    training_n = training_X.shape[0]\n",
    "    valid_rows = int(valid_percentage * training_n)\n",
    "\n",
    "    # create the validation set\n",
    "    valid_X = training_X.iloc[:valid_rows]\n",
    "    valid_y = training_y.iloc[:valid_rows]\n",
    "\n",
    "    # create the (smaller) training set\n",
    "    train_X = training_X.iloc[valid_rows:]\n",
    "    train_y = training_y.iloc[valid_rows:]\n",
    "\n",
    "    return train_X, train_y, valid_X, valid_y\n"
   ],
   "id": "6b9303710c42dad6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# trains a neural network with given training data\n",
    "def train_network(network, training_X, training_y, lr, is_classification, verbose=False):\n",
    "    # split the training data into train and validation\n",
    "    # Note: use 20% of the original training data for validation\n",
    "    train_X, train_y, valid_X, valid_y = create_validation(training_X, training_y, 0.2)\n",
    "\n",
    "    # convert our data to PyTorch objects\n",
    "    train_X = torch.from_numpy(train_X.values).long()\n",
    "    train_y = torch.from_numpy(train_y.values).long()\n",
    "    valid_X = torch.from_numpy(valid_X.values).long()\n",
    "    valid_y = torch.from_numpy(valid_y.values).long()\n",
    "\n",
    "    # move the data and model to the GPU if possible\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "        print(f\"Using device {torch.cuda.get_device_name(device)}\")\n",
    "        train_X = train_X.to(device)\n",
    "        train_y = train_y.to(device)\n",
    "        valid_X = valid_X.to(device)\n",
    "        valid_y = valid_y.to(device)\n",
    "        network = network.to(device)\n",
    "\n",
    "    # create the algorithm that learns the weight for the network\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    # create the loss function that tells optimizer how much error it has in its predictions\n",
    "    # here we use cross entropy since we have a classification task with more than two possible labels\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # train for 1000 epochs\n",
    "    num_epochs = 1000\n",
    "    train_loss_values = []\n",
    "    valid_loss_values = []\n",
    "    train_acc_values = []\n",
    "    valid_acc_values = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # make predictions on the training set and validation set\n",
    "        train_predictions = network(train_X)\n",
    "        valid_predictions = network(valid_X)\n",
    "        train_loss = loss_function(train_predictions, train_y)\n",
    "\n",
    "        # calculate the error on the training set\n",
    "        train_loss_values.append(train_loss.item())\n",
    "        valid_loss_values.append(loss_function(valid_predictions, valid_y).item())\n",
    "        train_acc_values.append(calculate_accuracy(network, train_X, train_y))\n",
    "        valid_acc = calculate_accuracy(network, valid_X, valid_y)\n",
    "        valid_acc_values.append(valid_acc)\n",
    "\n",
    "        # Early return for perfect fit\n",
    "        if valid_acc == 1:\n",
    "            # convert the training progress data to a Pandas DataFrame\n",
    "            progress = {\n",
    "                \"epoch\": range(epoch+1),\n",
    "                \"train_loss\": train_loss_values,\n",
    "                \"valid_loss\": valid_loss_values,\n",
    "                \"train_acc\": train_acc_values,\n",
    "                \"valid_acc\": valid_acc_values\n",
    "            }\n",
    "            return pandas.DataFrame(progress)\n",
    "\n",
    "        # perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # convert the training progress data to a Pandas DataFrame\n",
    "    progress = {\n",
    "        \"epoch\": range(num_epochs),\n",
    "        \"train_loss\": train_loss_values,\n",
    "        \"valid_loss\": valid_loss_values,\n",
    "        \"train_acc\": train_acc_values,\n",
    "        \"valid_acc\": valid_acc_values\n",
    "    }\n",
    "\n",
    "    return pandas.DataFrame(progress)\n"
   ],
   "id": "730514a3de38a90c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T02:50:16.686518Z",
     "start_time": "2024-12-17T02:50:16.683852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataset(dataset_filepath, input_size):\n",
    "    max_size = min_line_size(dataset_filepath)\n",
    "    if input_size + 3 > max_size:\n",
    "        raise ValueError(f\"Input size {input_size} greater than maximum allowed size {max_size}\")\n",
    "\n",
    "    # We need to create a dataframe with the proper number of columns\n",
    "    # Get the first three tokens from every line and store them in front of each instance\n",
    "    dataframe_dict = {\n",
    "        'timesig': [],\n",
    "        'key': [],\n",
    "        'style': []\n",
    "    }\n",
    "\n",
    "    for i in range(input_size):\n",
    "        dataframe_dict[i] = []\n",
    "\n",
    "    dataset_fp = open(dataset_filepath, 'r')\n",
    "    for line in dataset_fp.readlines():\n",
    "        line = line.split(',')\n",
    "        timesig = line[0]\n",
    "        key = line[1]\n",
    "        style = line[2]\n",
    "        remainder = line[3:]\n",
    "        for i in range(len(remainder) - input_size):\n",
    "            window = remainder[i:i+input_size]\n",
    "            dataframe_dict['timesig'].append(timesig)\n",
    "            dataframe_dict['key'].append(key)\n",
    "            dataframe_dict['style'].append(style)\n",
    "            for ind, token in enumerate(window):\n",
    "                dataframe_dict[ind].append(token)\n",
    "\n",
    "    dataset_fp.close()\n",
    "    return pandas.DataFrame(dataframe_dict)\n",
    "\n"
   ],
   "id": "498faa1729079a0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset_fp = open(DATASET, 'r')\n",
    "translation_fp = open(TRANSLATION_FILE, 'r')\n",
    "\n",
    "# Load translation into indexable array\n",
    "translation = []\n",
    "for line in translation_fp.readlines():\n",
    "    translation.append(line)\n",
    "num_tokens = len(translation)\n",
    "\n",
    "\n",
    "\n",
    "# Now split each line of input into all possible slid windows and add them\n",
    "\n",
    "dataset_fp.close()\n",
    "translation_fp.close()"
   ],
   "id": "15633f7b6c1407e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T02:52:34.940632Z",
     "start_time": "2024-12-17T02:52:34.903238Z"
    }
   },
   "cell_type": "code",
   "source": "process_dataset(DATASET, INPUT_SIZE)",
   "id": "507cfcdfee29c60b",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input size 5 greater than maximum allowed size 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mprocess_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATASET\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mINPUT_SIZE\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m, in \u001B[0;36mprocess_dataset\u001B[0;34m(dataset_filepath, input_size)\u001B[0m\n\u001B[1;32m      2\u001B[0m max_size \u001B[38;5;241m=\u001B[39m min_line_size(dataset_filepath)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m input_size \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;241m>\u001B[39m max_size:\n\u001B[0;32m----> 4\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minput_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m greater than maximum allowed size \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# We need to create a dataframe with the proper number of columns\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Get the first three tokens from every line and store them in front of each instance\u001B[39;00m\n\u001B[1;32m      8\u001B[0m dataframe_dict \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimesig\u001B[39m\u001B[38;5;124m'\u001B[39m: [],\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkey\u001B[39m\u001B[38;5;124m'\u001B[39m: [],\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstyle\u001B[39m\u001B[38;5;124m'\u001B[39m: []\n\u001B[1;32m     12\u001B[0m }\n",
      "\u001B[0;31mValueError\u001B[0m: Input size 5 greater than maximum allowed size 2"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
